# "Batch Normalizeation: Accelaeratiing Deep Network Training by Reducing Internal Covariate Shift"

## *训练加速

## 摘要
---
Training Deep Nerural Networks is complicated by the fact theat the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization

1. 提出了internal covaiate shift现象

2. **当使用特定的训练集训练好了用于识别的神经网络后，针对具体的分割定位任务进行fine-tune, 可以获得显著的效果提升(when labeled training data are scarce,supervised pre-training for an auxilliary task, followed by domain-specific fine-tune,boosts performance significantly)**
---

## 1.论文的主要贡献：
---
