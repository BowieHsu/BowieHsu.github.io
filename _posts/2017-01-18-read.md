# "Show, Attend and Tell:Neural Image Caption Generation with VIsual Attention"

## *主要应用于机器翻译与物体识别

---

## 1.论文的主要贡献：
---
- soft deterministic attention mechanism trainable by standard back-propagation
- hard stochastic attention mechanism trainable by maximizing an approximate variantional lower bound or equivalently by reinforce
- we show how we can gain insight and interpret the results of this framework by visualizing "where" and "what" the attention focused on
- we quantitatively validate the usefulness of attention in caption generation with state of the art performance

## 2.相关工作
---
- first approach to use neural networks for caption generation was kiros,who proposed a multimodal log-bilinear model that was biased by features from the image
- all of those works represent images as a single feature vector from the top layer of a pre-trained convolutional network.\
- prior to the use of neural networks for generating captions, two main approaches were dominant:\
- 1.the first involved generating caption templates which were filled in based on the results of object detections and attribute discovery\
- 2.the second approach was based on first retrieving similar captioned images from a large database then modifying these retrieved captions to fit the query.

## 3.使用 attention 机制进行图像语义生成

---
### 3.1 使用Convolutional features进行编码

- our model takes a single raw image and generates a caption y encoded as a sequence of 1-of-K encoded words.
- In order to obtain a correspondence between the feature vectors and portions of the 2-D image, we extract features from a lower convolutional layer unlike previous work which instead used a fully connected layer.


### 3.2 使用LSTM进行解码
- 待补    

## 4."Hard" Attention 与 "Soft" Attention
---
### 4.1 Stochastic "Hard" Attention

We represent the location variable st as where the model decides to focus attention when generating the t(th) word.
st,i is an indicator one-hot variable which is set to 1 if the i-th location is the one used to extract visual features.
soft 损失函数
需要迭代地计算几个训练权重
### 4.2 Deterministic "Soft" Attention

### 4.3 Training Procedure
*Adam algorithm*



## 5.实验
---
### 5.1 数据

**1.使用Flickr8k以及Flickr30k.**

**2.匹配结果使用BLEU metric表示.**

### 5.2 实验效果提升（Evaluation Procedures）

- 选择不同的卷积特征提取网络带来的效果差异
- 单模型vs组合模型效果
- 不同的数据分类效果对比

### 5.3 结果表格
---
|    dataset         |    Best Model    |    BLEU-1     | BLEU-2 |  BLEU-3  |  BLEU-4  |  BLEU-5  |
|--------------------| ---------------- |---------------|--------|----------|----------|----------|
|    Flickr8k        |  Hard-Attention  |     63        |   45.7 |  31.4    |   21.3   |   20.3   |
|    Flickr30k       |  Hard-Attention  |     66.9      |  43.9  |  29.6    |   19.9   |   18.46  |
|    COCO            |  Hard-Attention  |     71.8      |   50.4 |  35.7    |   25.0   |  23.04   |
