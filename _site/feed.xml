<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom"><generator uri="http://jekyllrb.com" version="3.1.6">Jekyll</generator><link href="/feed.xml" rel="self" type="application/atom+xml" /><link href="/" rel="alternate" type="text/html" /><updated>2017-02-27T13:00:21-05:00</updated><id>/</id><title>Site Title</title><subtitle>Describe your website here.</subtitle><entry><title>Rcnn原理解释</title><link href="/rcnn%E5%8E%9F%E7%90%86%E8%A7%A3%E9%87%8A/" rel="alternate" type="text/html" title="Rcnn原理解释" /><published>2017-02-03T00:00:00-05:00</published><updated>2017-02-03T00:00:00-05:00</updated><id>/rcnn原理解释</id><content type="html" xml:base="/rcnn%E5%8E%9F%E7%90%86%E8%A7%A3%E9%87%8A/">&lt;h1 id=&quot;region-based-convolutional-networks-for-accurate-obeject-detection-and-segmentation&quot;&gt;“Region-based Convolutional Networks for Accurate Obeject Detection and Segmentation”&lt;/h1&gt;

&lt;h2 id=&quot;section&quot;&gt;*应用于物体定位与识别&lt;/h2&gt;

&lt;h2 id=&quot;section-1&quot;&gt;摘要&lt;/h2&gt;
&lt;hr /&gt;
&lt;p&gt;由于历史原因，目前物体识别算法在PASCAL VOC测试上的良好表现均源自于使用由多种低级图像描述子组成的复杂检测系统。
在本篇论文中，我们提出一种简单且通用的识别算法，以期改进识别的mean average precision(mAP)。
我们的论文由两个基本观念组成：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;我们使用卷积神经网络自底向上地建立区域概率来完成目标的定位和分割工作
(one can apply high-capacity convolutional networks(CNN) to bottom-up region proposals in order to localize and segment obejcts)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;h2 id=&quot;fine-tune-when-labeled-training-data-are-scarcesupervised-pre-training-for-an-auxilliary-task-followed-by-domain-specific-fine-tuneboosts-performance-significantly&quot;&gt;&lt;strong&gt;当使用特定的训练集训练好了用于识别的神经网络后，针对具体的分割定位任务进行fine-tune, 可以获得显著的效果提升(when labeled training data are scarce,supervised pre-training for an auxilliary task, followed by domain-specific fine-tune,boosts performance significantly)&lt;/strong&gt;&lt;/h2&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;section-2&quot;&gt;1.论文的主要贡献：&lt;/h2&gt;
&lt;hr /&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;与图像分类不同，检测需要物体的定位信息。一个有效地设想是把图像检测当做一个回归问题(regression problem)来考虑，这样的方案可以有效地检测到单个物体，但在识别多个物体时，需要复杂的额外定位工作。另外一个设想是建立滑窗检测器(sliding-window detector)，卷积神经网络在近二十年内都是被用作滑窗检测器来进行人脸、手、行人检测，这种方案计算效率较高，但是它需要检测目标具有相同的长宽比(aspect ratio)，长宽比问题可以使用混合模型(mixture model)方法来解决。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;本文使用“区域识别”方法(recognition using regions)来解决定位问题。在识别过程中，我们生成了2000类相互独立的可能区域(region proposals),然后使用线性SVMs进行分类。我们使用图像缩放技术(anisotropic image scaling)来将输入到CNN中的图像固定成相同尺寸。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;本文解决的第二个问题是利用fine_tune进行定位效果的提升这一流程的建立
&lt;img src=&quot;../images/rcnn.jpg&quot; alt=&quot;rcnn_algorithm&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section-3&quot;&gt;2.相关工作&lt;/h2&gt;
&lt;hr /&gt;
&lt;ul&gt;
  &lt;li&gt;Deep CNNs for object detection.&lt;/li&gt;
  &lt;li&gt;Scalbility and speed.&lt;/li&gt;
  &lt;li&gt;Localization methods.&lt;/li&gt;
  &lt;li&gt;Transfer learning.&lt;/li&gt;
  &lt;li&gt;R-CNN extensions.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section-4&quot;&gt;3.模块设计&lt;/h2&gt;

&lt;hr /&gt;
&lt;p&gt;### 3.1 Module design&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;为了计算可能区域，我们首先需要利用CNN来快捷方便地生成特征。我们使用矩形框将目标区域标出并归一化成一定的长宽。
In order to compute features for a region proposal, we must first convert the image data in that region into a form that is compatibel with the CNN,
its architecture requires inputs of a fixed S * S pixel size, we warp all pixels in a tight bounding box around it to the requires size.&lt;/li&gt;
  &lt;li&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;test-time-detection&quot;&gt;3.2 Test-time detection&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;我们使用前向神经网络在2000个可能区域提取特征，针对每一个类图像，我们使用SVM根据神经网络生成的特征训练判别模型，随后我们使用非极大值抑制获得最佳结果区域候选框。
We warp each proposal and forward propagate it through the CNN in order to compute features.Then, for each class, we score each extracted feature vector using the SVM trained for that class.Given all scored regions in an images, we apply a greedy non-maximum suppression that rejects a region if it has an intersection-over-union overlap with a higher scoring selected region larger than a learned threshold.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;使用RCNN进行定位识别任务有如下几个优势&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;ol&gt;
  &lt;li&gt;所有类别的生成特征神经网络共享同一套参数 all CNN parameters are shared across all categories&lt;/li&gt;
  &lt;li&gt;使用CNN得到的特征与其他方法获得的特征相比具有更低的维度 Second the feature vectors computed by the CNN are low-dimensional when compared to other common approaches.&lt;/li&gt;
  &lt;li&gt;在当前系统中需要关心的参数是 SVM权重 以及 非极大值抑制, 其中 CNN生成的特征矩阵为 2000 * 4096 而 SVM 权重矩阵为 4096 * N，N为定位任务的类别数
  分析指出，RCNN可以识别出上千种类型(analysis shows that R-CNNs can scale to thousands of object classed without resorting to approximate thechniques.)&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;trainning&quot;&gt;3.3 Trainning&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;Supervised pre-training
    &lt;ul&gt;
      &lt;li&gt;我们使用ILSVRC2012分类挑战的数据进行预训练。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Domain-specific fine-tuning
    &lt;ul&gt;
      &lt;li&gt;为了将CNNC应用到detection任务中，我们在预训练的神经网络上，输入目标的warped region proposals，使用随机梯度下降法来更新网络的参数。除了将ImageNet 1000分类的softmax层替换成随机初始化的（N+1）分类层，整个神经网络的结构不变。例如在VOC中N=20，而在ILSVRC2013中N=200。&lt;/li&gt;
      &lt;li&gt;在训练过程中，我们将于目标区域有0.5以上IoU的区域视为正样本，背景区域识为负样本。&lt;/li&gt;
      &lt;li&gt;SGD的学习率为0.001，在每一次SGD迭代过程中，采样32个正样本和96个负样本，用来构成128个样本的mini-batch，这样安排比例是因为有效区域的面积是远小于背景区域面积的，而针对VGG网络，由于其网络参数较多，运用缓存较大，可适当减少mini-batch的个数&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;section-5&quot;&gt;4.分析&lt;/h2&gt;
&lt;hr /&gt;

&lt;h2 id=&quot;ilsvrc2013-detection-dataset&quot;&gt;5.ILSVRC2013 DETECTION DATASET&lt;/h2&gt;
&lt;hr /&gt;

&lt;h3 id=&quot;section-6&quot;&gt;5.3 结果表格&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;../images/table.jpg&quot; alt=&quot;rcnn_algorithm&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;semantic-segmentation&quot;&gt;6 SEMANTIC SEGMENTATION&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;我们针对CPMC区域使用了三种策略：
    &lt;ol&gt;
      &lt;li&gt;第一种策略，忽略区域形状，直接根据形变后的区域计算CNN特征，但这个方法不适用于非矩形区域&lt;/li&gt;
      &lt;li&gt;第二种策略，根据区域的前景掩膜计算CNN特征，我们将背景区域替换成预先计算好的均值，所以在进行了均值归一化后，背景的值为0。(background regions are zero after mean subtraction)&lt;/li&gt;
      &lt;li&gt;第三种策略，结合了第一种和第二种策略。&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section-7&quot;&gt;7 实现细节&lt;/h2&gt;</content><summary>“Region-based Convolutional Networks for Accurate Obeject Detection and Segmentation”</summary></entry><entry><title>基于图文转换的ocr识别</title><link href="/%E5%9F%BA%E4%BA%8E%E5%9B%BE%E6%96%87%E8%BD%AC%E6%8D%A2%E7%9A%84OCR%E8%AF%86%E5%88%AB/" rel="alternate" type="text/html" title="基于图文转换的ocr识别" /><published>2017-01-17T00:00:00-05:00</published><updated>2017-01-17T00:00:00-05:00</updated><id>/基于图文转换的OCR识别</id><content type="html" xml:base="/%E5%9F%BA%E4%BA%8E%E5%9B%BE%E6%96%87%E8%BD%AC%E6%8D%A2%E7%9A%84OCR%E8%AF%86%E5%88%AB/">&lt;h1 id=&quot;region-based-convolutional-networks-for-accurate-obeject-detection-and-segmentation&quot;&gt;“Region-based Convolutional Networks for Accurate Obeject Detection and Segmentation”&lt;/h1&gt;

&lt;h2 id=&quot;section&quot;&gt;*应用于物体定位与识别&lt;/h2&gt;

&lt;h2 id=&quot;section-1&quot;&gt;摘要&lt;/h2&gt;
&lt;hr /&gt;
&lt;p&gt;由于历史原因，目前物体识别算法在PASCAL VOC测试上的良好表现均源自于使用由多种低级图像描述子组成的复杂检测系统。
在本篇论文中，我们提出一种简单且通用的识别算法，以期改进识别的mean average precision(mAP)。
我们的论文由两个基本观念组成：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;我们使用卷积神经网络自底向上地建立区域概率来完成目标的定位和分割工作
(one can apply high-capacity convolutional networks(CNN) to bottom-up region proposals in order to localize and segment obejcts)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;h2 id=&quot;fine-tune-when-labeled-training-data-are-scarcesupervised-pre-training-for-an-auxilliary-task-followed-by-domain-specific-fine-tuneboosts-performance-significantly&quot;&gt;&lt;strong&gt;当使用特定的训练集训练好了用于识别的神经网络后，针对具体的分割定位任务进行fine-tune, 可以获得显著的效果提升(when labeled training data are scarce,supervised pre-training for an auxilliary task, followed by domain-specific fine-tune,boosts performance significantly)&lt;/strong&gt;&lt;/h2&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;section-2&quot;&gt;1.论文的主要贡献：&lt;/h2&gt;
&lt;hr /&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;与图像分类不同，检测需要物体的定位信息。一个有效地设想是把图像检测当做一个回归问题(regression problem)来考虑，这样的方案可以有效地检测到单个物体，但在识别多个物体时，需要复杂的额外定位工作。另外一个设想是建立滑窗检测器(sliding-window detector)，卷积神经网络在近二十年内都是被用作滑窗检测器来进行人脸、手、行人检测，这种方案计算效率较高，但是它需要检测目标具有相同的长宽比(aspect ratio)，长宽比问题可以使用混合模型(mixture model)方法来解决。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;本文使用“区域识别”方法(recognition using regions)来解决定位问题。在识别过程中，我们生成了2000类相互独立的可能区域(region proposals),然后使用线性SVMs进行分类。我们使用图像缩放技术(anisotropic image scaling)来将输入到CNN中的图像固定成相同尺寸。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;本文解决的第二个问题是利用fine_tune进行定位效果的提升这一流程的建立
&lt;img src=&quot;../images/rcnn.jpg&quot; alt=&quot;rcnn_algorithm&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section-3&quot;&gt;2.相关工作&lt;/h2&gt;
&lt;hr /&gt;
&lt;ul&gt;
  &lt;li&gt;Deep CNNs for object detection.&lt;/li&gt;
  &lt;li&gt;Scalbility and speed.&lt;/li&gt;
  &lt;li&gt;Localization methods.&lt;/li&gt;
  &lt;li&gt;Transfer learning.&lt;/li&gt;
  &lt;li&gt;R-CNN extensions.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section-4&quot;&gt;3.模块设计&lt;/h2&gt;

&lt;hr /&gt;
&lt;p&gt;### 3.1 Module design&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;为了计算可能区域，我们首先需要利用CNN来快捷方便地生成特征。我们使用矩形框将目标区域标出并归一化成一定的长宽。
In order to compute features for a region proposal, we must first convert the image data in that region into a form that is compatibel with the CNN,
its architecture requires inputs of a fixed S * S pixel size, we warp all pixels in a tight bounding box around it to the requires size.&lt;/li&gt;
  &lt;li&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;test-time-detection&quot;&gt;3.2 Test-time detection&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;我们使用前向神经网络在2000个可能区域提取特征，针对每一个类图像，我们使用SVM根据神经网络生成的特征训练判别模型，随后我们使用非极大值抑制获得最佳结果区域候选框。
We warp each proposal and forward propagate it through the CNN in order to compute features.Then, for each class, we score each extracted feature vector using the SVM trained for that class.Given all scored regions in an images, we apply a greedy non-maximum suppression that rejects a region if it has an intersection-over-union overlap with a higher scoring selected region larger than a learned threshold.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;使用RCNN进行定位识别任务有如下几个优势&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;ol&gt;
  &lt;li&gt;所有类别的生成特征神经网络共享同一套参数 all CNN parameters are shared across all categories&lt;/li&gt;
  &lt;li&gt;使用CNN得到的特征与其他方法获得的特征相比具有更低的维度 Second the feature vectors computed by the CNN are low-dimensional when compared to other common approaches.&lt;/li&gt;
  &lt;li&gt;在当前系统中需要关心的参数是 SVM权重 以及 非极大值抑制, 其中 CNN生成的特征矩阵为 2000 * 4096 而 SVM 权重矩阵为 4096 * N，N为定位任务的类别数
  分析指出，RCNN可以识别出上千种类型(analysis shows that R-CNNs can scale to thousands of object classed without resorting to approximate thechniques.)&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;trainning&quot;&gt;3.3 Trainning&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;Supervised pre-training
    &lt;ul&gt;
      &lt;li&gt;我们使用ILSVRC2012分类挑战的数据进行预训练。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Domain-specific fine-tuning
    &lt;ul&gt;
      &lt;li&gt;为了将CNNC应用到detection任务中，我们在预训练的神经网络上，输入目标的warped region proposals，使用随机梯度下降法来更新网络的参数。除了将ImageNet 1000分类的softmax层替换成随机初始化的（N+1）分类层，整个神经网络的结构不变。例如在VOC中N=20，而在ILSVRC2013中N=200。&lt;/li&gt;
      &lt;li&gt;在训练过程中，我们将于目标区域有0.5以上IoU的区域视为正样本，背景区域识为负样本。&lt;/li&gt;
      &lt;li&gt;SGD的学习率为0.001，在每一次SGD迭代过程中，采样32个正样本和96个负样本，用来构成128个样本的mini-batch，这样安排比例是因为有效区域的面积是远小于背景区域面积的，而针对VGG网络，由于其网络参数较多，运用缓存较大，可适当减少mini-batch的个数&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;section-5&quot;&gt;4.分析&lt;/h2&gt;
&lt;hr /&gt;

&lt;h2 id=&quot;ilsvrc2013-detection-dataset&quot;&gt;5.ILSVRC2013 DETECTION DATASET&lt;/h2&gt;
&lt;hr /&gt;

&lt;h3 id=&quot;section-6&quot;&gt;5.3 结果表格&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;../images/table.jpg&quot; alt=&quot;rcnn_algorithm&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;semantic-segmentation&quot;&gt;6 SEMANTIC SEGMENTATION&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;我们针对CPMC区域使用了三种策略：
    &lt;ol&gt;
      &lt;li&gt;第一种策略，忽略区域形状，直接根据形变后的区域计算CNN特征，但这个方法不适用于非矩形区域&lt;/li&gt;
      &lt;li&gt;第二种策略，根据区域的前景掩膜计算CNN特征，我们将背景区域替换成预先计算好的均值，所以在进行了均值归一化后，背景的值为0。(background regions are zero after mean subtraction)&lt;/li&gt;
      &lt;li&gt;第三种策略，结合了第一种和第二种策略。&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section-7&quot;&gt;7 实现细节&lt;/h2&gt;</content><summary>“Region-based Convolutional Networks for Accurate Obeject Detection and Segmentation”</summary></entry><entry><title>Cnn进行图文转换</title><link href="/cnn%E8%BF%9B%E8%A1%8C%E5%9B%BE%E6%96%87%E8%BD%AC%E6%8D%A2/" rel="alternate" type="text/html" title="Cnn进行图文转换" /><published>2017-01-10T00:00:00-05:00</published><updated>2017-01-10T00:00:00-05:00</updated><id>/cnn进行图文转换</id><content type="html" xml:base="/cnn%E8%BF%9B%E8%A1%8C%E5%9B%BE%E6%96%87%E8%BD%AC%E6%8D%A2/">&lt;h1 id=&quot;show-attend-and-tellneural-image-caption-generation-with-visual-attention&quot;&gt;“Show, Attend and Tell:Neural Image Caption Generation with VIsual Attention”&lt;/h1&gt;

&lt;h2 id=&quot;section&quot;&gt;*主要应用于机器翻译与物体识别&lt;/h2&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;section-1&quot;&gt;1.论文的主要贡献：&lt;/h2&gt;
&lt;hr /&gt;
&lt;ul&gt;
  &lt;li&gt;soft deterministic attention mechanism trainable by standard back-propagation&lt;/li&gt;
  &lt;li&gt;hard stochastic attention mechanism trainable by maximizing an approximate variantional lower bound or equivalently by reinforce&lt;/li&gt;
  &lt;li&gt;we show how we can gain insight and interpret the results of this framework by visualizing “where” and “what” the attention focused on&lt;/li&gt;
  &lt;li&gt;we quantitatively validate the usefulness of attention in caption generation with state of the art performance&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section-2&quot;&gt;2.相关工作&lt;/h2&gt;
&lt;hr /&gt;
&lt;ul&gt;
  &lt;li&gt;first approach to use neural networks for caption generation was kiros,who proposed a multimodal log-bilinear model that was biased by features from the image&lt;/li&gt;
  &lt;li&gt;all of those works represent images as a single feature vector from the top layer of a pre-trained convolutional network.\&lt;/li&gt;
  &lt;li&gt;prior to the use of neural networks for generating captions, two main approaches were dominant:\&lt;/li&gt;
  &lt;li&gt;1.the first involved generating caption templates which were filled in based on the results of object detections and attribute discovery\&lt;/li&gt;
  &lt;li&gt;2.the second approach was based on first retrieving similar captioned images from a large database then modifying these retrieved captions to fit the query.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;attention-&quot;&gt;3.使用 attention 机制进行图像语义生成&lt;/h2&gt;

&lt;hr /&gt;
&lt;p&gt;### 3.1 使用Convolutional features进行编码&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;our model takes a single raw image and generates a caption y encoded as a sequence of 1-of-K encoded words.&lt;/li&gt;
  &lt;li&gt;In order to obtain a correspondence between the feature vectors and portions of the 2-D image, we extract features from a lower convolutional layer unlike previous work which instead used a fully connected layer.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;lstm&quot;&gt;3.2 使用LSTM进行解码&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;待补&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;hard-attention--soft-attention&quot;&gt;4.”Hard” Attention 与 “Soft” Attention&lt;/h2&gt;
&lt;hr /&gt;
&lt;p&gt;### 4.1 Stochastic “Hard” Attention&lt;/p&gt;

&lt;p&gt;We represent the location variable st as where the model decides to focus attention when generating the t(th) word.
st,i is an indicator one-hot variable which is set to 1 if the i-th location is the one used to extract visual features.
soft 损失函数
需要迭代地计算几个训练权重
### 4.2 Deterministic “Soft” Attention&lt;/p&gt;

&lt;h3 id=&quot;training-procedure&quot;&gt;4.3 Training Procedure&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Adam algorithm&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;section-3&quot;&gt;5.实验&lt;/h2&gt;
&lt;hr /&gt;
&lt;p&gt;### 5.1 数据&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;1.使用Flickr8k以及Flickr30k.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;2.匹配结果使用BLEU metric表示.&lt;/strong&gt;&lt;/p&gt;

&lt;h3 id=&quot;evaluation-procedures&quot;&gt;5.2 实验效果提升（Evaluation Procedures）&lt;/h3&gt;</content><summary>“Show, Attend and Tell:Neural Image Caption Generation with VIsual Attention”</summary></entry><entry><title>Prml 6</title><link href="/prml-6/" rel="alternate" type="text/html" title="Prml 6" /><published>2016-05-11T00:00:00-04:00</published><updated>2016-05-11T00:00:00-04:00</updated><id>/prml-6</id><content type="html" xml:base="/prml-6/">&lt;p&gt;“Chapter6-7 Kernel Methods and SVM”&lt;/p&gt;

&lt;!--使用MathJax编辑latex公式--&gt;
&lt;script type=&quot;text/javascript&quot; src=&quot;http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML&quot;&gt;
&lt;/script&gt;

&lt;p&gt;##核函数技巧与支持向量基&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;section&quot;&gt;1.前言&lt;/h2&gt;
&lt;hr /&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;在第三、四章中，我们详细地介绍了如何使用线性模型来进行分类和回归模型的学习，在这种学习过程中，一组训练数据通常用来获得点估计或者后验概率分布，整个训练是为了获得参数w，这种学习方法也适用于像神经网络这种非线性学习模型。
&lt;em&gt;(In Chapters 3 and 4, we considered linear parametric models for regression and classification in which the form of the mapping y(x,w) from input x to output y is governed by a vector w of adaptive parameters.During the learning parse,a set of training data is used either to obtain a point estimate of the parameter vector or to determine a posterior distribution over this vector. The training data is then discarded, and predictions for new inputs are based purely on the learned parameter vector w.This approach is also used in nonlinear parametric models such as neural networks.)&lt;/em&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;除了上述的学习方法外，我们将接触一种新的学习方法，它通过保留训练过程中的有效数据集来形成判断函数，这种方法也蕴含在之前的最近邻判别方法中，都是使用已知的数据集来判断未来数据集的方法。这种方法通常需要一个矩阵来计算测试数据与训练数据之间的相似性，从而实现对测试数据的判断。&lt;em&gt;( However, there is a class of pattern recognition techniques,in which the training data points,or a subset of them, are kept and used also during the prediction phase.For instance, the Parzen probability density model comprised a linear combination of ‘kernel’ functions each one centered on one of the training data points.These are the examples of memory-based methods that involve storing the entire training set in order to make predictions for future ddata points.)&lt;/em&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;!--$$ evidence\_{i}=\sum \_{j}W\_{ij}x\_{j}+b\_{i} $$--&gt;

&lt;h2 id=&quot;section-1&quot;&gt;2. 对偶表达式&lt;/h2&gt;
&lt;hr /&gt;
&lt;ul&gt;
  &lt;li&gt;我们定义一个参数由最小均方误差定义的线性回归模型:&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;\frac{1}{2}\sum_{n=1}^{N}{(w^T\phi(x_n) - t_n)}^2 + \frac{\lambda}{2}w^tw&lt;/script&gt;
&lt;script type=&quot;math/tex&quot;&gt;{\lambda &gt;= 0}&lt;/script&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;其中\(\phi\)是设计出来的矩阵，向量\(\alpha = (\alpha_1,\dot)\)是设计用来替代\w\的参数，因此我们得到新的均方误差损失函数&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;对偶表达式的优势在于完全将表达式替换成了只与k有关的形式：&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;y(x) = w^t\phi(x) = \alpha^T\Phi\phi(x) =  k(x)^T{(K + \lambda{I_N})}^{-1}{t}&lt;/script&gt;

&lt;h2 id=&quot;section-2&quot;&gt;3. 构造核函数&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;../images/kernels.jpg&quot; alt=&quot;constructing_kernels&quot; /&gt;
—&lt;/p&gt;

&lt;h2 id=&quot;section-3&quot;&gt;4. 基于高斯过程的回归&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;In order to apply Gasussian process models to the problem of regression, we need to take account of the noise on the observeed target values, which are given by &lt;script type=&quot;math/tex&quot;&gt;t_n = y_n + {\epsilon}_n&lt;/script&gt;, 其中\(y_n\)是函数输出，\(\epsilon\)是随机噪声变量&lt;/li&gt;
  &lt;li&gt;
    &lt;table&gt;
      &lt;tbody&gt;
        &lt;tr&gt;
          &lt;td&gt;当我们考虑噪声为高斯分布时，有$$p(t_n&lt;/td&gt;
          &lt;td&gt;y_n)=\mathcal{N}(y&lt;/td&gt;
          &lt;td&gt;0,K)$$&lt;/td&gt;
        &lt;/tr&gt;
      &lt;/tbody&gt;
    &lt;/table&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><summary>“Chapter6-7 Kernel Methods and SVM”</summary></entry><entry><title>Syntax Highlighting Post</title><link href="/code-highlighting-post/" rel="alternate" type="text/html" title="Syntax Highlighting Post" /><published>2013-08-16T00:00:00-04:00</published><updated>2013-08-16T00:00:00-04:00</updated><id>/code-highlighting-post</id><content type="html" xml:base="/code-highlighting-post/">&lt;p&gt;Syntax highlighting is a feature that displays source code, in different colors and fonts according to the category of terms. This feature facilitates writing in a structured language such as a programming language or a markup language as both structures and syntax errors are visually distinct. Highlighting does not affect the meaning of the text itself; it is intended only for human readers.&lt;sup id=&quot;fnref:1&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;h3 id=&quot;gfm-code-blocks&quot;&gt;GFM Code Blocks&lt;/h3&gt;

&lt;p&gt;GitHub Flavored Markdown &lt;a href=&quot;https://help.github.com/articles/creating-and-highlighting-code-blocks/&quot;&gt;fenced code blocks&lt;/a&gt; are supported. To modify styling and highlight colors edit &lt;code class=&quot;highlighter-rouge&quot;&gt;/_sass/syntax.scss&lt;/code&gt;.&lt;/p&gt;

&lt;div class=&quot;language-css highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nf&quot;&gt;#container&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;nl&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;left&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
  &lt;span class=&quot;nl&quot;&gt;margin&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;0&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;-240px&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;0&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
  &lt;span class=&quot;nl&quot;&gt;width&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;100%&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-scss&quot; data-lang=&quot;scss&quot;&gt;&lt;span class=&quot;nc&quot;&gt;.highlight&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;nl&quot;&gt;margin&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
  &lt;span class=&quot;nl&quot;&gt;padding&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;1em&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
  &lt;span class=&quot;nl&quot;&gt;font-family&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$monospace&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
  &lt;span class=&quot;nl&quot;&gt;font-size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$type-size-7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
  &lt;span class=&quot;nl&quot;&gt;line-height&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;.8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;div class=&quot;language-html highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nt&quot;&gt;&amp;lt;nav&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;class=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;pagination&quot;&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;role=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;navigation&quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&lt;/span&gt;
  {% if page.previous %}
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;a&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;href=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;{{ site.url }}{{ page.previous.url }}&quot;&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;class=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;btn&quot;&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;title=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;{{ page.previous.title }}&quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&lt;/span&gt;Previous article&lt;span class=&quot;nt&quot;&gt;&amp;lt;/a&amp;gt;&lt;/span&gt;
  {% endif %}
  {% if page.next %}
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;a&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;href=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;{{ site.url }}{{ page.next.url }}&quot;&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;class=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;btn&quot;&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;title=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;{{ page.next.title }}&quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&lt;/span&gt;Next article&lt;span class=&quot;nt&quot;&gt;&amp;lt;/a&amp;gt;&lt;/span&gt;
  {% endif %}
&lt;span class=&quot;nt&quot;&gt;&amp;lt;/nav&amp;gt;&lt;/span&gt;&lt;span class=&quot;c&quot;&gt;&amp;lt;!-- /.pagination --&amp;gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-html&quot; data-lang=&quot;html&quot;&gt;&lt;table style=&quot;border-spacing: 0&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;gutter gl&quot; style=&quot;text-align: right&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;nt&quot;&gt;&amp;lt;nav&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;class=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;pagination&quot;&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;role=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;navigation&quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&lt;/span&gt;
  {% if page.previous %}
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;a&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;href=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;{{ site.url }}{{ page.previous.url }}&quot;&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;class=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;btn&quot;&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;title=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;{{ page.previous.title }}&quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&lt;/span&gt;Previous article&lt;span class=&quot;nt&quot;&gt;&amp;lt;/a&amp;gt;&lt;/span&gt;
  {% endif %}
  {% if page.next %}
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;a&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;href=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;{{ site.url }}{{ page.next.url }}&quot;&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;class=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;btn&quot;&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;title=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;{{ page.next.title }}&quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&lt;/span&gt;Next article&lt;span class=&quot;nt&quot;&gt;&amp;lt;/a&amp;gt;&lt;/span&gt;
  {% endif %}
&lt;span class=&quot;nt&quot;&gt;&amp;lt;/nav&amp;gt;&lt;/span&gt;&lt;span class=&quot;c&quot;&gt;&amp;lt;!-- /.pagination --&amp;gt;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;div class=&quot;language-ruby highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;module&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;Jekyll&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;TagIndex&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;Page&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;initialize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;site&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;base&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dir&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tag&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
      &lt;span class=&quot;vi&quot;&gt;@site&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;site&lt;/span&gt;
      &lt;span class=&quot;vi&quot;&gt;@base&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;base&lt;/span&gt;
      &lt;span class=&quot;vi&quot;&gt;@dir&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dir&lt;/span&gt;
      &lt;span class=&quot;vi&quot;&gt;@name&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&#39;index.html&#39;&lt;/span&gt;
      &lt;span class=&quot;nb&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;process&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;vi&quot;&gt;@name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
      &lt;span class=&quot;nb&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;read_yaml&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;no&quot;&gt;File&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;join&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;base&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&#39;_layouts&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&#39;tag_index.html&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
      &lt;span class=&quot;nb&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&#39;tag&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tag&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;tag_title_prefix&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;site&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&#39;tag_title_prefix&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;||&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&#39;Tagged: &#39;&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;tag_title_suffix&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;site&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&#39;tag_title_suffix&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;||&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&#39;&amp;amp;#8211;&#39;&lt;/span&gt;
      &lt;span class=&quot;nb&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&#39;title&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;#{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tag_title_prefix&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}#{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tag&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;
      &lt;span class=&quot;nb&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&#39;description&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;An archive of posts tagged &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;#{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tag&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;.&quot;&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;code-blocks-in-lists&quot;&gt;Code Blocks in Lists&lt;/h3&gt;

&lt;p&gt;Indentation matters. Be sure the indent of the code block aligns with the first non-space character after the list item marker (e.g., &lt;code class=&quot;highlighter-rouge&quot;&gt;1.&lt;/code&gt;). Usually this will mean indenting 3 spaces instead of 4.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Do step 1.&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Now do this:&lt;/p&gt;

    &lt;div class=&quot;language-ruby highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;print_hi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;nb&quot;&gt;puts&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;Hi, &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;#{&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;print_hi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&#39;Tom&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;#=&amp;gt; prints &#39;Hi, Tom&#39; to STDOUT.&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;Now you can do this.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;github-gist-embed&quot;&gt;GitHub Gist Embed&lt;/h3&gt;

&lt;p&gt;An example of a Gist embed below.&lt;/p&gt;

&lt;noscript&gt;&lt;pre&gt;404: Not Found
&lt;/pre&gt;&lt;/noscript&gt;
&lt;script src=&quot;https://gist.github.com/mmistakes/6589546.js&quot;&gt; &lt;/script&gt;

&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;http://en.wikipedia.org/wiki/Syntax_highlighting&quot;&gt;http://en.wikipedia.org/wiki/Syntax_highlighting&lt;/a&gt; &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><category term="sample post" /><category term="code" /><category term="highlighting" /><summary>Syntax highlighting is a feature that displays source code, in different colors and fonts according to the category of terms. This feature facilitates writing in a structured language such as a programming language or a markup language as both structures and syntax errors are visually distinct. Highlighting does not affect the meaning of the text itself; it is intended only for human readers.1


  
    
      http://en.wikipedia.org/wiki/Syntax_highlighting &amp;#8617;</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="{&quot;feature&quot;=&gt;&quot;abstract-10.jpg&quot;, &quot;credit&quot;=&gt;&quot;dargadgetz&quot;, &quot;creditlink&quot;=&gt;&quot;http://www.dargadgetz.com/ios-7-abstract-wallpaper-pack-for-iphone-5-and-ipod-touch-retina/&quot;}" /></entry><entry><title>Sample Link Post</title><link href="/sample-link-post/" rel="alternate" type="text/html" title="Sample Link Post" /><published>2013-08-12T00:00:00-04:00</published><updated>2013-08-12T00:00:00-04:00</updated><id>/sample-link-post</id><content type="html" xml:base="/sample-link-post/">&lt;p&gt;This theme supports &lt;strong&gt;link posts&lt;/strong&gt;, made famous by John Gruber. To use, just add &lt;code class=&quot;highlighter-rouge&quot;&gt;link: http://url-you-want-linked&lt;/code&gt; to the post’s YAML front matter and you’re done.&lt;/p&gt;</content><category term="sample post" /><category term="link post" /><summary>This theme supports link posts, made famous by John Gruber. To use, just add link: http://url-you-want-linked to the post’s YAML front matter and you’re done.</summary></entry><entry><title>Testing Readability with a Bunch of Text</title><link href="/readability-post/" rel="alternate" type="text/html" title="Testing Readability with a Bunch of Text" /><published>2012-05-22T00:00:00-04:00</published><updated>2012-05-22T00:00:00-04:00</updated><id>/readability-post</id><content type="html" xml:base="/readability-post/">&lt;p&gt;Portland in shoreditch Vice, labore typewriter pariatur hoodie fap sartorial Austin. Pinterest literally occupy Schlitz forage. Odio ad blue bottle vinyl, 90’s narwhal commodo bitters pour-over nostrud. Ugh est hashtag in, fingerstache adipisicing laboris esse Pinterest shabby chic Portland. Shoreditch bicycle rights anim, flexitarian laboris put a bird on it vinyl cupidatat narwhal. Hashtag artisan skateboard, flannel Bushwick nesciunt salvia aute fixie do plaid post-ironic dolor McSweeney’s. Cliche pour-over chambray nulla four loko skateboard sapiente hashtag.&lt;/p&gt;

&lt;p&gt;Vero laborum commodo occupy. Semiotics voluptate mumblecore pug. Cosby sweater ullamco quinoa ennui assumenda, sapiente occupy delectus lo-fi. Ea fashion axe Marfa cillum aliquip. Retro Bushwick keytar cliche. Before they sold out sustainable gastropub Marfa readymade, ethical Williamsburg skateboard brunch qui consectetur gentrify semiotics. Mustache cillum irony, fingerstache magna pour-over keffiyeh tousled selfies.&lt;/p&gt;

&lt;h2 id=&quot;cupidatat-90s-lo-fi-authentic-try-hard&quot;&gt;Cupidatat 90’s lo-fi authentic try-hard&lt;/h2&gt;

&lt;p&gt;In pug Portland incididunt mlkshk put a bird on it vinyl quinoa. Terry Richardson shabby chic +1, scenester Tonx excepteur tempor fugiat voluptate fingerstache aliquip nisi next level. Farm-to-table hashtag Truffaut, Odd Future ex meggings gentrify single-origin coffee try-hard 90’s.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Sartorial hoodie&lt;/li&gt;
  &lt;li&gt;Labore viral forage&lt;/li&gt;
  &lt;li&gt;Tote bag selvage&lt;/li&gt;
  &lt;li&gt;DIY exercitation et id ugh tumblr church-key&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Incididunt umami sriracha, ethical fugiat VHS ex assumenda yr irure direct trade. Marfa Truffaut bicycle rights, kitsch placeat Etsy kogi asymmetrical. Beard locavore flexitarian, kitsch photo booth hoodie plaid ethical readymade leggings yr.&lt;/p&gt;

&lt;p&gt;Aesthetic odio dolore, meggings disrupt qui readymade stumptown brunch Terry Richardson pour-over gluten-free. Banksy american apparel in selfies, biodiesel flexitarian organic meh wolf quinoa gentrify banjo kogi. Readymade tofu ex, scenester dolor umami fingerstache occaecat fashion axe Carles jean shorts minim. Keffiyeh fashion axe nisi Godard mlkshk dolore. Lomo you probably haven’t heard of them eu non, Odd Future Truffaut pug keytar meggings McSweeney’s Pinterest cred. Etsy literally aute esse, eu bicycle rights qui meggings fanny pack. Gentrify leggings pug flannel duis.&lt;/p&gt;

&lt;h2 id=&quot;forage-occaecat-cardigan-qui&quot;&gt;Forage occaecat cardigan qui&lt;/h2&gt;

&lt;p&gt;Fashion axe hella gastropub lo-fi kogi 90’s aliquip +1 veniam delectus tousled. Cred sriracha locavore gastropub kale chips, iPhone mollit sartorial. Anim dolore 8-bit, pork belly dolor photo booth aute flannel small batch. Dolor disrupt ennui, tattooed whatever salvia Banksy sartorial roof party selfies raw denim sint meh pour-over. Ennui eu cardigan sint, gentrify iPhone cornhole.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Whatever velit occaecat quis deserunt gastropub, leggings elit tousled roof party 3 wolf moon kogi pug blue bottle ea. Fashion axe shabby chic Austin quinoa pickled laborum bitters next level, disrupt deep v accusamus non fingerstache.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Tote bag asymmetrical elit sunt. Occaecat authentic Marfa, hella McSweeney’s next level irure veniam master cleanse. Sed hoodie letterpress artisan wolf leggings, 3 wolf moon commodo ullamco. Anim occupy ea labore Terry Richardson. Tofu ex master cleanse in whatever pitchfork banh mi, occupy fugiat fanny pack Austin authentic. Magna fugiat 3 wolf moon, labore McSweeney’s sustainable vero consectetur. Gluten-free disrupt enim, aesthetic fugiat jean shorts trust fund keffiyeh magna try-hard.&lt;/p&gt;

&lt;h2 id=&quot;hoodie-duis&quot;&gt;Hoodie Duis&lt;/h2&gt;

&lt;p&gt;Actually salvia consectetur, hoodie duis lomo YOLO sunt sriracha. Aute pop-up brunch farm-to-table odio, salvia irure occaecat. Sriracha small batch literally skateboard. Echo Park nihil hoodie, aliquip forage artisan laboris. Trust fund reprehenderit nulla locavore. Stumptown raw denim kitsch, keffiyeh nulla twee dreamcatcher fanny pack ullamco 90’s pop-up est culpa farm-to-table. Selfies 8-bit do pug odio.&lt;/p&gt;

&lt;h3 id=&quot;thundercats-ho&quot;&gt;Thundercats Ho!&lt;/h3&gt;

&lt;p&gt;Fingerstache thundercats Williamsburg, deep v scenester Banksy ennui vinyl selfies mollit biodiesel duis odio pop-up. Banksy 3 wolf moon try-hard, sapiente enim stumptown deep v ad letterpress. Squid beard brunch, exercitation raw denim yr sint direct trade. Raw denim narwhal id, flannel DIY McSweeney’s seitan. Letterpress artisan bespoke accusamus, meggings laboris consequat Truffaut qui in seitan. Sustainable cornhole Schlitz, twee Cosby sweater banh mi deep v forage letterpress flannel whatever keffiyeh. Sartorial cred irure, semiotics ethical sed blue bottle nihil letterpress.&lt;/p&gt;

&lt;p&gt;Occupy et selvage squid, pug brunch blog nesciunt hashtag mumblecore skateboard yr kogi. Ugh small batch swag four loko. Fap post-ironic qui tote bag farm-to-table american apparel scenester keffiyeh vero, swag non pour-over gentrify authentic pitchfork. Schlitz scenester lo-fi voluptate, tote bag irony bicycle rights pariatur vero Vice freegan wayfarers exercitation nisi shoreditch. Chambray tofu vero sed. Street art swag literally leggings, Cosby sweater mixtape PBR lomo Banksy non in pitchfork ennui McSweeney’s selfies. Odd Future Banksy non authentic.&lt;/p&gt;

&lt;p&gt;Aliquip enim artisan dolor post-ironic. Pug tote bag Marfa, deserunt pour-over Portland wolf eu odio intelligentsia american apparel ugh ea. Sunt viral et, 3 wolf moon gastropub pug id. Id fashion axe est typewriter, mlkshk Portland art party aute brunch. Sint pork belly Cosby sweater, deep v mumblecore kitsch american apparel. Try-hard direct trade tumblr sint skateboard. Adipisicing bitters excepteur biodiesel, pickled gastropub aute veniam.&lt;/p&gt;</content><category term="sample post" /><category term="readability" /><category term="test" /><category term="intro" /><summary>Portland in shoreditch Vice, labore typewriter pariatur hoodie fap sartorial Austin. Pinterest literally occupy Schlitz forage. Odio ad blue bottle vinyl, 90’s narwhal commodo bitters pour-over nostrud. Ugh est hashtag in, fingerstache adipisicing laboris esse Pinterest shabby chic Portland. Shoreditch bicycle rights anim, flexitarian laboris put a bird on it vinyl cupidatat narwhal. Hashtag artisan skateboard, flannel Bushwick nesciunt salvia aute fixie do plaid post-ironic dolor McSweeney’s. Cliche pour-over chambray nulla four loko skateboard sapiente hashtag.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="{&quot;feature&quot;=&gt;&quot;abstract-6.jpg&quot;, &quot;credit&quot;=&gt;&quot;dargadgetz&quot;, &quot;creditlink&quot;=&gt;&quot;http://www.dargadgetz.com/ios-7-abstract-wallpaper-pack-for-iphone-5-and-ipod-touch-retina/&quot;}" /></entry><entry><title>Sample Post</title><link href="/sample-post/" rel="alternate" type="text/html" title="Sample Post" /><published>2011-03-10T00:00:00-05:00</published><updated>2011-03-10T00:00:00-05:00</updated><id>/sample-post</id><content type="html" xml:base="/sample-post/">&lt;p&gt;Below is just about everything you’ll need to style in the theme. Check the source code to see the many embedded elements within paragraphs.&lt;/p&gt;

&lt;h1 id=&quot;section&quot;&gt;第一章&lt;/h1&gt;

&lt;h2 id=&quot;heading-2&quot;&gt;Heading 2&lt;/h2&gt;

&lt;h3 id=&quot;heading-3&quot;&gt;Heading 3&lt;/h3&gt;

&lt;h4 id=&quot;heading-4&quot;&gt;Heading 4&lt;/h4&gt;

&lt;h5 id=&quot;heading-5&quot;&gt;Heading 5&lt;/h5&gt;

&lt;h6 id=&quot;heading-6&quot;&gt;Heading 6&lt;/h6&gt;

&lt;h3 id=&quot;body-text&quot;&gt;Body text&lt;/h3&gt;

&lt;p&gt;Lorem ipsum dolor sit amet, test link adipiscing elit. &lt;strong&gt;This is strong&lt;/strong&gt;. Nullam dignissim convallis est. Quisque aliquam.&lt;/p&gt;

&lt;p class=&quot;image-right&quot;&gt;&lt;img src=&quot;/images/3953273590_704e3899d5_m.jpg&quot; alt=&quot;Smithsonian Image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;This is emphasized&lt;/em&gt;. Donec faucibus. Nunc iaculis suscipit dui. 53 = 125. Water is H&lt;sub&gt;2&lt;/sub&gt;O. Nam sit amet sem. Aliquam libero nisi, imperdiet at, tincidunt nec, gravida vehicula, nisl. The New York Times &lt;cite&gt;(That’s a citation)&lt;/cite&gt;. &lt;u&gt;Underline&lt;/u&gt;. Maecenas ornare tortor. Donec sed tellus eget sapien fringilla nonummy. Mauris a ante. Suspendisse quam sem, consequat at, commodo vitae, feugiat in, nunc. Morbi imperdiet augue quis tellus.&lt;/p&gt;

&lt;p&gt;HTML and &lt;abbr title=&quot;cascading stylesheets&quot;&gt;CSS&lt;abbr&gt; are our tools. Mauris a ante. Suspendisse quam sem, consequat at, commodo vitae, feugiat in, nunc. Morbi imperdiet augue quis tellus. Praesent mattis, massa quis luctus fermentum, turpis mi volutpat justo, eu volutpat enim diam eget metus.&lt;/abbr&gt;&lt;/abbr&gt;&lt;/p&gt;

&lt;h3 id=&quot;blockquotes&quot;&gt;Blockquotes&lt;/h3&gt;

&lt;blockquote&gt;
  &lt;p&gt;Lorem ipsum dolor sit amet, test link adipiscing elit. Nullam dignissim convallis est. Quisque aliquam.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;list-types&quot;&gt;List Types&lt;/h2&gt;

&lt;h3 id=&quot;ordered-lists&quot;&gt;Ordered Lists&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;Item one
    &lt;ol&gt;
      &lt;li&gt;sub item one&lt;/li&gt;
      &lt;li&gt;sub item two&lt;/li&gt;
      &lt;li&gt;sub item three&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;Item two&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;unordered-lists&quot;&gt;Unordered Lists&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Item one&lt;/li&gt;
  &lt;li&gt;Item two&lt;/li&gt;
  &lt;li&gt;Item three&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;tables&quot;&gt;Tables&lt;/h2&gt;

&lt;table rules=&quot;groups&quot;&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;Header1&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Header2&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;Header3&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;cell1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;cell2&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;cell3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;cell4&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;cell5&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;cell6&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;cell1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;cell2&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;cell3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;cell4&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;cell5&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;cell6&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
  &lt;tfoot&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Foot1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Foot2&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;Foot3&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tfoot&gt;
&lt;/table&gt;

&lt;h2 id=&quot;code-snippets&quot;&gt;Code Snippets&lt;/h2&gt;

&lt;p&gt;Syntax highlighting via Rouge&lt;/p&gt;

&lt;div class=&quot;language-css highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nf&quot;&gt;#container&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;nl&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;left&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
  &lt;span class=&quot;nl&quot;&gt;margin&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;0&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;-240px&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;0&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
  &lt;span class=&quot;nl&quot;&gt;width&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;100%&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Non Pygments code example&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;lt;div id=&quot;awesome&quot;&amp;gt;
    &amp;lt;p&amp;gt;This is great isn&#39;t it?&amp;lt;/p&amp;gt;
&amp;lt;/div&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h2 id=&quot;buttons&quot;&gt;Buttons&lt;/h2&gt;

&lt;p&gt;Make any link standout more when applying the &lt;code class=&quot;highlighter-rouge&quot;&gt;.btn&lt;/code&gt; class.&lt;/p&gt;

&lt;div class=&quot;language-html highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nt&quot;&gt;&amp;lt;a&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;href=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;#&quot;&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;class=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;btn btn-success&quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&lt;/span&gt;Success Button&lt;span class=&quot;nt&quot;&gt;&amp;lt;/a&amp;gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div&gt;&lt;a href=&quot;#&quot; class=&quot;btn&quot;&gt;Primary Button&lt;/a&gt;&lt;/div&gt;
&lt;div&gt;&lt;a href=&quot;#&quot; class=&quot;btn btn-success&quot;&gt;Success Button&lt;/a&gt;&lt;/div&gt;
&lt;div&gt;&lt;a href=&quot;#&quot; class=&quot;btn btn-warning&quot;&gt;Warning Button&lt;/a&gt;&lt;/div&gt;
&lt;div&gt;&lt;a href=&quot;#&quot; class=&quot;btn btn-danger&quot;&gt;Danger Button&lt;/a&gt;&lt;/div&gt;
&lt;div&gt;&lt;a href=&quot;#&quot; class=&quot;btn btn-info&quot;&gt;Info Button&lt;/a&gt;&lt;/div&gt;</content><category term="sample post" /><summary>Below is just about everything you’ll need to style in the theme. Check the source code to see the many embedded elements within paragraphs.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="{&quot;feature&quot;=&gt;&quot;abstract-3.jpg&quot;, &quot;credit&quot;=&gt;&quot;dargadgetz&quot;, &quot;creditlink&quot;=&gt;&quot;http://www.dargadgetz.com/ios-7-abstract-wallpaper-pack-for-iphone-5-and-ipod-touch-retina/&quot;}" /></entry></feed>
