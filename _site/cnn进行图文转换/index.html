<h1 id="show-attend-and-tellneural-image-caption-generation-with-visual-attention">“Show, Attend and Tell:Neural Image Caption Generation with VIsual Attention”</h1>

<h2 id="section">*主要应用于机器翻译与物体识别</h2>

<hr />

<h2 id="section-1">1.论文的主要贡献：</h2>
<hr />
<ul>
  <li>soft deterministic attention mechanism trainable by standard back-propagation</li>
  <li>hard stochastic attention mechanism trainable by maximizing an approximate variantional lower bound or equivalently by reinforce</li>
  <li>we show how we can gain insight and interpret the results of this framework by visualizing “where” and “what” the attention focused on</li>
  <li>we quantitatively validate the usefulness of attention in caption generation with state of the art performance</li>
</ul>

<h2 id="section-2">2.相关工作</h2>
<hr />
<ul>
  <li>first approach to use neural networks for caption generation was kiros,who proposed a multimodal log-bilinear model that was biased by features from the image</li>
  <li>all of those works represent images as a single feature vector from the top layer of a pre-trained convolutional network.\</li>
  <li>prior to the use of neural networks for generating captions, two main approaches were dominant:\</li>
  <li>1.the first involved generating caption templates which were filled in based on the results of object detections and attribute discovery\</li>
  <li>2.the second approach was based on first retrieving similar captioned images from a large database then modifying these retrieved captions to fit the query.</li>
</ul>

<h2 id="attention-">3.使用 attention 机制进行图像语义生成</h2>

<hr />
<p>### 3.1 使用Convolutional features进行编码</p>

<ul>
  <li>our model takes a single raw image and generates a caption y encoded as a sequence of 1-of-K encoded words.</li>
  <li>In order to obtain a correspondence between the feature vectors and portions of the 2-D image, we extract features from a lower convolutional layer unlike previous work which instead used a fully connected layer.</li>
</ul>

<h3 id="lstm">3.2 使用LSTM进行解码</h3>
<ul>
  <li>待补</li>
</ul>

<h2 id="hard-attention--soft-attention">4.”Hard” Attention 与 “Soft” Attention</h2>
<hr />
<p>### 4.1 Stochastic “Hard” Attention</p>

<p>We represent the location variable st as where the model decides to focus attention when generating the t(th) word.
st,i is an indicator one-hot variable which is set to 1 if the i-th location is the one used to extract visual features.
soft 损失函数
需要迭代地计算几个训练权重
### 4.2 Deterministic “Soft” Attention</p>

<h3 id="training-procedure">4.3 Training Procedure</h3>
<p><em>Adam algorithm</em></p>

<h2 id="section-3">5.实验</h2>
<hr />
<p>### 5.1 数据</p>

<p><strong>1.使用Flickr8k以及Flickr30k.</strong></p>

<p><strong>2.匹配结果使用BLEU metric表示.</strong></p>

<h3 id="evaluation-procedures">5.2 实验效果提升（Evaluation Procedures）</h3>
