<p>“Chapter6-7 Kernel Methods and SVM”</p>

<!--使用MathJax编辑latex公式-->
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

<p>##核函数技巧与支持向量基</p>

<hr />

<h2 id="section">1.前言</h2>
<hr />
<ul>
  <li>
    <p>在第三、四章中，我们详细地介绍了如何使用线性模型来进行分类和回归模型的学习，在这种学习过程中，一组训练数据通常用来获得点估计或者后验概率分布，整个训练是为了获得参数w，这种学习方法也适用于像神经网络这种非线性学习模型。
<em>(In Chapters 3 and 4, we considered linear parametric models for regression and classification in which the form of the mapping y(x,w) from input x to output y is governed by a vector w of adaptive parameters.During the learning parse,a set of training data is used either to obtain a point estimate of the parameter vector or to determine a posterior distribution over this vector. The training data is then discarded, and predictions for new inputs are based purely on the learned parameter vector w.This approach is also used in nonlinear parametric models such as neural networks.)</em></p>
  </li>
  <li>
    <p>除了上述的学习方法外，我们将接触一种新的学习方法，它通过保留训练过程中的有效数据集来形成判断函数，这种方法也蕴含在之前的最近邻判别方法中，都是使用已知的数据集来判断未来数据集的方法。这种方法通常需要一个矩阵来计算测试数据与训练数据之间的相似性，从而实现对测试数据的判断。<em>( However, there is a class of pattern recognition techniques,in which the training data points,or a subset of them, are kept and used also during the prediction phase.For instance, the Parzen probability density model comprised a linear combination of ‘kernel’ functions each one centered on one of the training data points.These are the examples of memory-based methods that involve storing the entire training set in order to make predictions for future ddata points.)</em></p>
  </li>
</ul>

<!--$$ evidence\_{i}=\sum \_{j}W\_{ij}x\_{j}+b\_{i} $$-->

<h2 id="section-1">2. 对偶表达式</h2>
<hr />
<ul>
  <li>我们定义一个参数由最小均方误差定义的线性回归模型:</li>
</ul>

<p><script type="math/tex">\frac{1}{2}\sum_{n=1}^{N}{(w^T\phi(x_n) - t_n)}^2 + \frac{\lambda}{2}w^tw</script>
<script type="math/tex">{\lambda >= 0}</script></p>

<ul>
  <li>
    <p>其中\(\phi\)是设计出来的矩阵，向量\(\alpha = (\alpha_1,\dot)\)是设计用来替代\w\的参数，因此我们得到新的均方误差损失函数</p>
  </li>
  <li>
    <p>对偶表达式的优势在于完全将表达式替换成了只与k有关的形式：</p>
  </li>
</ul>

<script type="math/tex; mode=display">y(x) = w^t\phi(x) = \alpha^T\Phi\phi(x) =  k(x)^T{(K + \lambda{I_N})}^{-1}{t}</script>

<h2 id="section-2">3. 构造核函数</h2>
<p><img src="../images/kernels.jpg" alt="constructing_kernels" />
—</p>

<h2 id="section-3">4. 基于高斯过程的回归</h2>

<ul>
  <li>In order to apply Gasussian process models to the problem of regression, we need to take account of the noise on the observeed target values, which are given by <script type="math/tex">t_n = y_n + {\epsilon}_n</script>, 其中\(y_n\)是函数输出，\(\epsilon\)是随机噪声变量</li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>当我们考虑噪声为高斯分布时，有$$p(t_n</td>
          <td>y_n)=\mathcal{N}(y</td>
          <td>0,K)$$</td>
        </tr>
      </tbody>
    </table>
  </li>
</ul>

